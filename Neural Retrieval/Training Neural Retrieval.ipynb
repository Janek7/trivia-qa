{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Training Neural Retrieval.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"kCOpW3ezdut5"},"source":["# Reranking/Retrieval with RoBERTa"]},{"cell_type":"markdown","metadata":{"id":"L8F6f-Pabbe4"},"source":["This notebook contains two approaches on how to fine-tune RoBERTa for neural re-ranking. In the \"Training\" section the models are trained with huggingface Trainer. \n","\n","The following section contains code to train the same with pytorch lightning. This code is experimental. It wraps the training in pytorch lightning module. This enables us to evaluate with MAP during training and to increase the rate of difficult samples during training with the function described in the report. \n","\n","Due to limited time we could not finally finish this implementation. Therefore we trained with huggingface trainer and evaluated the models at checkpoints after 250 training steps with MAP. The evaluation for this is implemented in Bert_Reranking_EVAL.ipynb"]},{"cell_type":"markdown","metadata":{"id":"dxmKwtBLXv48"},"source":["## Imports"]},{"cell_type":"code","metadata":{"id":"qiW06XFjRArv"},"source":["!pip install pytorch-lightning\n","!pip install transformers datasets pickle5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2m38XoR2ekg2"},"source":["from google.colab import drive\n","import pickle5 as pickle\n","from transformers import RobertaForSequenceClassification, Trainer, TrainingArguments\n","import torch\n","from torch.utils.data import IterableDataset, DataLoader\n","import pytorch_lightning as pl\n","from pytorch_lightning.callbacks import ModelCheckpoint\n","import datetime\n","import os\n","import numpy as np\n","import time\n","from datasets import load_dataset, Dataset, concatenate_datasets\n","from collections import Counter\n","import matplotlib.pyplot as plt\n","from torch.nn import functional as F\n","from IPython import display\n","import gc\n","import random\n","import math\n","from torchmetrics import RetrievalMAP"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DmZOgk0EFvBA"},"source":["## Load Files"]},{"cell_type":"code","metadata":{"id":"lKJeaPMubHzv"},"source":["def save_as_pickle(obj, filename):\n","    \"\"\"\n","    save an object in a pickle file dump\n","    :param obj: object to dump\n","    :param filename: target file\n","    :return:\n","    \"\"\"\n","    directory = os.path.dirname(filename)\n","    if not os.path.exists(directory):\n","        os.makedirs(directory)\n","    \n","    with open(filename, 'wb') as file:\n","        pickle.dump(obj, file, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","\n","def load_pickle(filename):\n","    \"\"\"\n","    load an object from a given pickle file\n","    :param filename: source file\n","    :return: loaded object\n","    \"\"\"\n","    with open(filename, 'rb') as file:\n","        return pickle.load(file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Dx-D-y-_g30"},"source":["drive.mount('/content/drive', force_remount=True)  # use force_remount=True param after upload of new data\n","\n","# file and folder names\n","ir_project_drive_folder = \"Master/2 - FSS 2021/Information Retrieval/IR Projekt\"\n","# ir_project_drive_folder = \"IR Projekt\"\n","full_ir_project_drive_folder = \"/content/drive/My Drive/{}/\".format(ir_project_drive_folder)\n","\n","# datasets\n","nr_datasets_folder = full_ir_project_drive_folder + '/data/wikipedia/neural-retrieval-datasets'\n","nr_train_relevant_set_filename = nr_datasets_folder + '/train_relevant'\n","nr_train_irrelevant_random_set_filename = nr_datasets_folder + '/train_irrelevant_random'\n","nr_train_irrelevant_hard_random_set_filename = nr_datasets_folder + '/train_irrelevant_hard_random'\n","nr_train_irrelevant_hard_max_set_filename = nr_datasets_folder + '/train_irrelevant_hard_max'\n","\n","# TODO: rework\n","nr_dev_relevant_set_filename = nr_datasets_folder + '/dev_relevant'\n","nr_dev_irrelevant_set_filename = nr_datasets_folder + '/dev_irrelevant'\n","nr_dev_irrelevant_smart_set_filename = nr_datasets_folder + '/dev_irrelevant_smart'\n","bm25_retrieved_dev_pl_map_filename = nr_datasets_folder + \"/dev_pl_map.pkl\"\n","\n","\n","# saved models\n","nr_models_folder = full_ir_project_drive_folder + 'saved_models/neural_retrieval'\n","\n","# first cycle (random irrelevant)\n","model_path_roberta_first_10000_10000_wd_2e = nr_models_folder + '/roberta_first_10000_10000_wd_2e'\n","model_path_roberta_first_5000_15000_wd_2e = nr_models_folder + '/roberta_first_5000_15000_wd_2e'\n","model_path_roberta_first_720_19280_wd_2e = nr_models_folder + '/roberta_first_720_19280_wd_2e'\n","\n","# second cycle (varying irrelevants)\n","model_path_roberta_second_5000_15000_wd_2e = nr_models_folder + '/roberta_second_5000_15000_wd_2e'\n","model_path_roberta_second_randhard_5000_15000_wd_2e = nr_models_folder + '/roberta_second_randhard_5000_15000_wd_2e'\n","model_path_roberta_second_maxhard_5000_15000_wd_2e = nr_models_folder + '/roberta_second_maxhard_5000_15000_wd_2e'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jBlG1NoCTZm8"},"source":["## Load Data"]},{"cell_type":"code","metadata":{"id":"U0XMEqJ1BiYO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623580528111,"user_tz":-120,"elapsed":415,"user":{"displayName":"Katharina Prasse","photoUrl":"","userId":"10798570393675516236"}},"outputId":"70ef49f1-aaf9-4688-8010-8d5833d902a3"},"source":["train_dataset_relevant = Dataset.load_from_disk(nr_train_relevant_set_filename)\n","#train_dataset_relevant = train_dataset_relevant.remove_columns(\"type\")\n","print(\"Train relevant:\", train_dataset_relevant)\n","\n","# dev_dataset_relevant = Dataset.load_from_disk(nr_dev_relevant_set_filename)\n","# print(\"Dev relevant:\", dev_dataset_relevant)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train relevant: Dataset({\n","    features: ['attention_mask', 'input_ids', 'label', 'question', 'relevant_document', 'type'],\n","    num_rows: 110516\n","})\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"b1mjcuXGBm7P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623580203743,"user_tz":-120,"elapsed":388,"user":{"displayName":"Katharina Prasse","photoUrl":"","userId":"10798570393675516236"}},"outputId":"73d59c34-19cc-4bbb-9382-66879f99702e"},"source":["# train_dataset_irrelevant_random = Dataset.load_from_disk(nr_train_irrelevant_random_set_filename)\n","# print(\"Train irrelevant random:\", train_dataset_irrelevant_random)\n","\n","#dev_dataset_irrelevant_hard = Dataset.load_from_disk(nr_dev_irrelevant_smart_set_filename)\n","#print(\"Dev irrelevant smart:\", dev_dataset_irrelevant_hard)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train irrelevant random: Dataset({\n","    features: ['attention_mask', 'input_ids', 'label', 'question', 'relevant_document', 'type'],\n","    num_rows: 110647\n","})\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tKV__PANZBN3"},"source":["# train_dataset_irrelevant_hard_random = Dataset.load_from_disk(nr_train_irrelevant_hard_random_set_filename)\n","# print(\"Train irrelevant hard random:\", train_dataset_irrelevant_hard_random)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dozKzD8f2yyy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623580534443,"user_tz":-120,"elapsed":891,"user":{"displayName":"Katharina Prasse","photoUrl":"","userId":"10798570393675516236"}},"outputId":"7c5e1213-2dc3-45e8-8f96-8e0e9df085dc"},"source":["train_dataset_irrelevant_hard_max = Dataset.load_from_disk(nr_train_irrelevant_hard_max_set_filename)\n","print(\"Train irrelevant hard max:\", train_dataset_irrelevant_hard_max)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train irrelevant hard max: Dataset({\n","    features: ['attention_mask', 'input_ids', 'label', 'question', 'relevant_document', 'type'],\n","    num_rows: 123772\n","})\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"D2w8BbW0V6eB"},"source":["def merge_datasets(relevant_dataset, irrelevant_dataset, number_relevants=None, number_irrelevants=None, shuffling=True):\n","  \"\"\" \n","  merges two datasets\n","  number_relevants: number of rows from first set to take\n","  number_irrelevants: number of rows from second set to take\n","  shuffling: flag if datasets should be shuffled before drawing samples\n","  \"\"\"\n","  # shuffle initial dataset\n","  if shuffling:\n","    relevant_dataset = relevant_dataset.shuffle(seed=40)\n","    irrelevant_dataset = irrelevant_dataset.shuffle(seed=40)\n","\n","  # limit rows\n","  if number_relevants:\n","    relevant_dataset = Dataset.from_dict(relevant_dataset[:number_relevants])\n","  if number_irrelevants:\n","    irrelevant_dataset = Dataset.from_dict(irrelevant_dataset[:number_irrelevants])\n","  \n","  # concat limited sets and shuffle again\n","  concatted = concatenate_datasets([relevant_dataset, irrelevant_dataset])\n","  if shuffling:\n","    concatted = concatted.shuffle(seed=40)\n","\n","  return concatted"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4zrEi045chZQ","executionInfo":{"status":"ok","timestamp":1623580637475,"user_tz":-120,"elapsed":97683,"user":{"displayName":"Katharina Prasse","photoUrl":"","userId":"10798570393675516236"}},"outputId":"36fd4e71-0207-4214-8e28-d8de870a6069"},"source":["train_dataset = merge_datasets(train_dataset_relevant, train_dataset_irrelevant_hard_max, number_relevants=5000, number_irrelevants=15000)\n","print(train_dataset)\n","#dev_dataset = merge_datasets(dev_dataset_relevant, dev_dataset_irrelevant, number_relevants=10000, number_irrelevants=10000)\n","#print(dev_dataset)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading cached shuffled indices for dataset at /content/drive/My Drive/Master/2 - FSS 2021/Information Retrieval/IR Projekt/data/wikipedia/neural-retrieval-datasets/train_relevant/cache-f442c7fd50de0853.arrow\n","Loading cached shuffled indices for dataset at /content/drive/My Drive/Master/2 - FSS 2021/Information Retrieval/IR Projekt/data/wikipedia/neural-retrieval-datasets/train_irrelevant_hard_max/cache-50f0e46b5001a7f0.arrow\n"],"name":"stderr"},{"output_type":"stream","text":["Dataset({\n","    features: ['attention_mask', 'input_ids', 'label', 'question', 'relevant_document', 'type'],\n","    num_rows: 20000\n","})\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wXERXZ9jUhc5"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"lkL46FmdewCw"},"source":["# model = RobertaForSequenceClassification.from_pretrained('roberta-base') \n","model = RobertaForSequenceClassification.from_pretrained(model_path_roberta_first_5000_15000_wd_2e + '/checkpoint-5000') "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XBO7Jx_m3LqP"},"source":["class RetrievalTrainer(Trainer):\n","\n","  def __init__(self, model_name, *args, **kwargs):\n","    super().__init__(*args, **kwargs)\n","    self.loss_graph_file = kwargs['args'].output_dir + '/loss_graph.png'\n","    self.logs_file = kwargs['args'].output_dir + '/training_logs.pkl'\n","    self.training_loss_graph_file = kwargs['args'].output_dir + '/training_loss_graph.png'\n","    self.model_name = model_name\n","\n","    self.current_training_loss = None\n","    self.losses = []\n","    self.training_losses = []\n","    self.training_logs = []\n","\n","  def log(self, logs):\n","    super().log(logs)\n","    if 'loss' in logs:\n","      self.training_logs.append(logs)\n","      self.current_training_loss = logs['loss']\n","      self.training_losses.append(logs['loss'])\n","      if logs['epoch'] % 0.1 == 0:\n","        save_as_pickle(self.training_logs, self.logs_file)\n","    elif 'eval_loss' in logs and self.current_training_loss:\n","      self.losses.append((logs['epoch'], self.current_training_loss, logs['eval_loss']))\n","    display.clear_output(wait=True)\n","    # self.plot_losses()\n","    self.plot_training_losses(save=False)\n","\n","  def plot_losses(self, save=True, show=True):\n","    epochs = [loss_log[0] for loss_log in self.losses]\n","    training_losses = [loss_log[1] for loss_log in self.losses]\n","    eval_losses = [loss_log[2] for loss_log in self.losses]\n","    plt.plot(epochs, training_losses, color='cornflowerblue',  label='Training Loss') \n","    plt.plot(epochs, eval_losses, color='coral', label='Evaluation Loss') \n","    plt.title(self.model_name)\n","    plt.legend(loc='upper right')\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    if save:\n","      plt.savefig(self.loss_graph_file)\n","    if show:\n","      plt.show()\n","\n","  def plot_training_losses(self, save=True, show=True):\n","    steps = [idx for idx, training_loss in enumerate(self.training_losses)]\n","    training_losses = [training_loss for training_loss in self.training_losses]\n","    plt.plot(steps, training_losses, color='cornflowerblue',  label='Training Loss') \n","    plt.title(self.model_name)\n","    plt.legend(loc='upper right')\n","    plt.xlabel(\"Steps\")\n","    plt.ylabel(\"Loss\")\n","    if save:\n","      plt.savefig(self.training_loss_graph_file)\n","    if show:\n","      plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nGR-2Qc6UZCM"},"source":["from transformers import TrainingArguments\n","\n","training_args = TrainingArguments(\n","    output_dir = model_path_roberta_second_maxhard_5000_15000_wd_2e,# output directory\n","    logging_dir = model_path_roberta_second_maxhard_5000_15000_wd_2e + '/logs',            # directory for storing logs\n","    num_train_epochs = 2,              # total number of training epochs\n","    per_device_train_batch_size = 8,  # batch size per device during training\n","    per_device_eval_batch_size = 8,   # batch size for evaluation\n","    weight_decay = 0.01,               # strength of weight decay\n","    learning_rate = 5e-6,\n","    logging_steps = 10,\n","    save_steps = 250,\n",")\n","\n","trainer = RetrievalTrainer(\n","    \"RoBERTa 5000 15000, Second Cycle, Max Hard\",\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-vsTe_NOncE4"},"source":["gc.collect()\n","torch.cuda.empty_cache()\n","trainer.train()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Sq3JEL_vMUc"},"source":["model.save_pretrained(model_path_roberta_second_maxhard_5000_15000_wd_2e)\n","save_as_pickle(trainer.training_logs, model_path_roberta_second_maxhard_5000_15000_wd_2e + '/training_logs.pkl')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AapDA23GLYXX"},"source":["## Experimental Training: Pytorch Lightning"]},{"cell_type":"markdown","metadata":{"id":"Re2Dvp_qcyJs"},"source":["### Data Module"]},{"cell_type":"code","metadata":{"id":"Jsaol92iLXLS"},"source":["class RetrievalDataModule(pl.LightningDataModule):\n","\n","  def __init__(self, train_relevant_dataset, train_irrelevant_dataset, train_irrelevant_hard_dataset,\n","                      dev_dataset, number_relevant=5000, number_irrelevant=5000, hard_factor_train=2.5):\n","    super().__init__()\n","    self.train_set = DynamicDataset(train_relevant_dataset, train_irrelevant_dataset, train_irrelevant_hard_dataset, number_relevant, number_irrelevant, hard_factor_train)\n","    self.dev_set = dev_dataset\n","  \n","  def prepare_data(self):\n","    \"\"\" download data \"\"\"\n","    pass\n","\n","  def setup(self, stage: str = None):\n","    \"\"\" set data based on phase \"\"\"\n","    pass\n","\n","  def train_dataloader(self):\n","    print(\"train data loader called\")\n","    return DataLoader(self.train_set, batch_size=8)\n","\n","  def val_dataloader(self):\n","    print(\"train data loader called\")\n","    return DataLoader(self.dev_set, batch_size=8)\n","\n","  def test_dataloader(self):\n","    pass\n","\n","\n","class DynamicDataset(torch.utils.data.Dataset):\n","\n","    def __init__(self, relevant_dataset, irrelevant_dataset, irrelevant_hard_dataset, \n","                  relevant_size, irrelevant_size, hard_factor=2.5):\n","      super().__init__()\n","      self.relevant_dataset = relevant_dataset\n","      self.irrelevant_dataset = irrelevant_dataset\n","      self.irrelevant_hard_dataset = irrelevant_hard_dataset\n","      \n","      self.whole_size = relevant_size + irrelevant_size\n","      self.relevant_size = relevant_size\n","      self.irrelevant_size = irrelevant_size\n","\n","      self.num_relevant = 0\n","      self.num_irrelevant = 0\n","      self.num_irrelevant_hard = 0\n","      \n","      self.hard_factor = hard_factor\n","      self.hard_probabilities = []\n","\n","    def log_train_set_distribution(self):\n","      print(f' Train set {self.whole_size} '.center(40, '-'))\n","      print('Relevant samples:', self.num_relevant)\n","      print('Irrelevant samples:', self.num_irrelevant)\n","      print('Irrelevant samples hard:', self.num_irrelevant_hard)\n","\n","    def __getitem__(self, idx):\n","      hard_probability = idx**self.hard_factor / self.__len__()**self.hard_factor\n","      self.hard_probabilities.append(hard_probability)\n","\n","      r = random.uniform(0, 1)\n","      relevant_threshold_upper = self.relevant_size / self.whole_size\n","\n","      irrelevant_hard_threshold_lower = 1 - hard_probability\n","\n","      irrelevant_threshold_lower = relevant_threshold_upper\n","      irrelevant_threshold_upper = irrelevant_hard_threshold_lower\n","\n","      if r < relevant_threshold_upper:\n","        r_int = random.randint(0, len(self.relevant_dataset) - 1)\n","        item = self.relevant_dataset.__getitem__(r_int)\n","        self.num_relevant += 1\n","      \n","      elif r > irrelevant_threshold_lower and r < irrelevant_threshold_upper:\n","        r_int = random.randint(0, len(self.irrelevant_dataset) - 1)\n","        item = self.irrelevant_dataset.__getitem__(r_int)\n","        self.num_irrelevant += 1\n","\n","      elif r > irrelevant_hard_threshold_lower:\n","        r_int = random.randint(0, len(self.irrelevant_hard_dataset) - 1)\n","        item = self.irrelevant_hard_dataset.__getitem__(r_int)\n","        self.num_irrelevant_hard += 1\n","      \n","      if 'type' in item:\n","        item.pop('type')\n","      \n","      return item\n","\n","    def __len__(self):\n","      return int(self.whole_size)\n","\n","    def plot_hard_probabilities(self):\n","      indexes = [i for i in range(len(self.hard_probabilities))]\n","      hard_probabilities = [p for p in self.hard_probabilities]\n","      plt.plot(indexes, hard_probabilities, color='cornflowerblue') \n","      plt.title(\"Share of hard samples in irrelevant samples\")\n","      plt.xlabel(\"Training Set Size\")\n","      plt.ylabel(\"Share\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X164pfjwO3Pr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623340170997,"user_tz":-120,"elapsed":1053,"user":{"displayName":"Janek Putz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5FJ98ILk_0Ap7WgkCYmcY0FRCrWotxy5Fyg28_Q=s64","userId":"03949637286954052197"}},"outputId":"b0cc467f-047d-4848-94c8-51790da5056e"},"source":["dm = RetrievalDataModule(train_dataset_relevant, train_dataset_irrelevant, train_dataset_irrelevant_hard, \n","                         dev_dataset, number_relevant=100, number_irrelevant=100, hard_factor_train=10)\n","for idx, batch in enumerate(dm.train_dataloader()):\n","  pass\n","  # print(batch['label'])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["train data loader called\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"id":"nTTDS4i0a6df","executionInfo":{"status":"ok","timestamp":1623340173008,"user_tz":-120,"elapsed":612,"user":{"displayName":"Janek Putz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5FJ98ILk_0Ap7WgkCYmcY0FRCrWotxy5Fyg28_Q=s64","userId":"03949637286954052197"}},"outputId":"c1a9f0be-830a-43d8-d025-e9a9060599db"},"source":["dm.train_set.log_train_set_distribution()\n","dm.train_set.plot_hard_probabilities()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["------------ Train set 200 -------------\n","Relevant samples: 94\n","Irrelevant samples: 89\n","Irrelevant samples hard: 17\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"JiwJsCsac0RH"},"source":["### Module"]},{"cell_type":"code","metadata":{"id":"3h5Jcn1FWm92"},"source":["class MapDataset(torch.utils.data.Dataset):\n","  def __init__(self, encodings):\n","      self.encodings = encodings\n","\n","  def __getitem__(self, idx):\n","      item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","      return item\n","\n","  def __len__(self):\n","      return len(self.encodings['input_ids'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WmzVW6uUcjmY"},"source":["class RetrievalModule(pl.LightningModule):\n","\n","  def __init__(self, model, model_desc, model_folder, data_module, map_encodings, lr=5e-5, wd=0.01):\n","    # learning rate and weight decay have same default values as in huggingface TrainerArguments\n","    super().__init__()\n","    self.model = model\n","    self.model_desc = model_desc\n","    self.data_module = data_module\n","    self.lr = lr\n","    self.wd = wd\n","\n","    self.training_losses = []\n","    self.loss_pairs = []  # (train_loss, val_loss)\n","    # list of losses of different batches from one validation\n","    self.current_val_batch_losses = []\n","\n","    if not os.path.exists(model_folder):\n","      os.makedirs(model_folder)\n","    self.training_loss_graph_file = model_folder + '/traing_loss.png'\n","    self.loss_graph_file = model_folder + '/graph.png'\n","\n","    # map dataset attributes\n","    self.map_indexes, self.map_targets, self.map_encodings = map_encodings\n","    self.huggingface_trainer = Trainer(model=model,args=TrainingArguments(output_dir=model_folder))#,disable_tqdm=False\n","\n","  def training_step(self, batch, batch_idx):\n","    \"\"\"\n","    compute training loss for given batch\n","    \"\"\"\n","    batch = self.transform_batch_for_bert(batch)\n","    outputs = self.model(**batch)\n","    loss = outputs[0]\n","    self.training_losses.append(loss.item())\n","    if batch_idx % 10 == 0:\n","      display.clear_output(wait=True)\n","      self.plot_training_losses()\n","      self.plot_losses()\n","      print(\"T Batch {} {} | train_loss avg: {} | train_loss: {}\".format(batch_idx, datetime.datetime.now(), np.mean(self.training_losses), loss))\n","    return loss\n","\n","  def validation_step(self, batch, batch_idx, dataloader_idx=0):\n","    \"\"\"\n","    perform a validation for a given batch\n","    \"\"\"\n","    if batch_idx % 1 == 0 and self.loss_pairs:\n","      print(\"V Batch\", batch_idx, datetime.datetime.now(), \n","            'val_loss avg:', np.mean([loss_pair[1] for loss_pair in self.loss_pairs]))\n","\n","    batch = self.transform_batch_for_bert(batch)\n","    outputs = self.model(**batch)\n","    loss, logits = outputs[:2]\n","    preds = torch.argmax(logits, axis=1)\n","    labels = batch[\"labels\"]\n","\n","    # save loss current batch in list to average it over all batches in one validation round\n","    self.current_val_batch_losses.append(loss.item())\n","\n","    return {'loss': loss, \"preds\": preds, \"labels\": labels}\n","\n","  def on_validation_end(self):\n","    \"\"\" \n","    print avg training and evaluation loss until now\n","    compute MAP\n","    plot loss graphs\n","    \"\"\"\n","    display.clear_output(wait=True)\n","    # break if training_losses are empty -> case for sanity check\n","    if not self.training_losses:\n","      return\n","    current_val_loss = np.mean(self.current_val_batch_losses)\n","    self.loss_pairs.append((self.training_losses[-1], \n","                            current_val_loss,\n","                            self.compute_map()))\n","    # clear batch validation losses for next validation\n","    self.current_val_batch_losses.clear()\n","\n","    \n","    if self.loss_pairs:\n","      self.plot_losses()\n","    if self.training_losses:\n","      self.plot_training_losses()\n","\n","    if self.training_losses:\n","      train_loss_mean = np.mean(self.training_losses)\n","      print('train_loss mean', train_loss_mean)\n","    if self.loss_pairs:\n","      val_loss_mean = np.mean([loss_pair[1] for loss_pair in self.loss_pairs])\n","      print('val_loss mean', val_loss_mean)\n","      print('last val_loss', current_val_loss)\n","    self.data_module.train_set.log_train_set_distribution()\n","    self.data_module.train_set.plot_hard_probabilities()\n","\n","  def configure_optimizers(self):\n","    return torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=self.wd)\n","\n","  @staticmethod\n","  def transform_batch_for_bert(batch):\n","    \"\"\"\n","    rename labels and transform input_ids to a to two dim tensor\n","    transpose dimensions: (512, batch_size) -> (batch_size, 512)\n","    \"\"\"\n","    # remove unnecessary fields\n","    if 'type' in batch:\n","      batch.pop('type')\n","    batch.pop('question')\n","    batch.pop('relevant_document')\n","\n","    # transform necessary fields\n","    batch['labels'] = batch.pop('label')\n","    batch['input_ids'] = torch.stack(batch.pop('input_ids')).transpose(0, 1)\n","    batch['attention_mask'] = torch.stack(batch.pop('attention_mask')).transpose(0, 1)\n","\n","    return batch\n","\n","  def plot_losses(self, save=True, show=True):\n","    val_steps = [i for i in range(len(self.loss_pairs))]\n","    training_losses = [loss_pair[0] for loss_pair in self.loss_pairs]\n","    eval_losses = [loss_pair[1] for loss_pair in self.loss_pairs]\n","    maps = [loss_pair[2] for loss_pair in self.loss_pairs]\n","    plt.plot(val_steps, training_losses, linestyle='solid', color='cornflowerblue',  label='Training Loss') \n","    plt.plot(val_steps, eval_losses, linestyle='dotted', color='coral', label='Evaluation Loss')\n","    plt.plot(val_steps, maps, linestyle='dashed', color='yellowgreen', label='MAP') \n","    plt.title(self.model_desc)\n","    plt.legend(loc='upper right')\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    if save:\n","      plt.savefig(self.loss_graph_file)\n","    if show:\n","      plt.show()\n","\n","  def plot_training_losses(self, save=True, show=True):\n","    batches = [i for i in range(len(self.training_losses))]\n","    training_losses = [loss for loss in self.training_losses]\n","    plt.plot(batches, training_losses, color='cornflowerblue',  label='Training Loss') \n","    plt.title(self.model_desc)\n","    plt.legend(loc='upper right')\n","    plt.xlabel(\"Batch\")\n","    plt.ylabel(\"Loss\")\n","    if save:\n","      plt.savefig(self.training_loss_graph_file)\n","    if show:\n","      plt.show()\n","\n","  def compute_map(self):\n","    retrieval_map = RetrievalMAP()\n","    prediction_output = self.huggingface_trainer.predict(MapDataset(self.map_encodings))\n","    display.clear_output(wait=True)\n","    predictions = prediction_output.predictions\n","    preds = torch.tensor(predictions)\n","    res = F.softmax(preds, dim = 1)\n","    res = res[:,1]\n","    map = retrieval_map(res, self.map_targets, indexes=self.map_indexes).item()\n","    return map"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mAnRTQMzeqyz"},"source":["map_encodings = load_pickle(bm25_retrieved_dev_pl_map_filename)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vIKzGqZGOinQ"},"source":["# del model\n","model = RobertaForSequenceClassification.from_pretrained('roberta-base') "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hQv_gu0KbUZY"},"source":["data_module = RetrievalDataModule(train_dataset_relevant, train_dataset_irrelevant, train_dataset_irrelevant_hard,\n","                                   dev_dataset, number_relevant=5000, number_irrelevant=5000, hard_factor_train=2.5)\n","retrieval_module = RetrievalModule(model, \n","                                   'RoBERTa 5000 5000, 2E, UEV, Incr. Hard (2.5)',\n","                                   model_path_roberta_pl_smart_5000_5000_wd_2e_25,\n","                                   data_module,\n","                                   map_encodings,\n","                                   lr=5e-5, wd=0.01)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"olcXHKQAc2iS"},"source":["### Training"]},{"cell_type":"code","metadata":{"id":"uYJs7uIx096e"},"source":["gc.collect()\n","torch.cuda.empty_cache()\n","\n","checkpoint_callback = ModelCheckpoint(\n","    dirpath=model_path_roberta_pl_smart_5000_5000_wd_2e_25,\n","    filename='Checkpoint-{step}',\n","    every_n_train_steps=500\n",")\n","\n","trainer = pl.Trainer(\n","    max_epochs=2,\n","    gpus=1,\n","    val_check_interval=0.25,  # checks 4 times per epoch\n","    callbacks=[checkpoint_callback],\n","    num_sanity_val_steps=0)\n","\n","# start timer\n","start = datetime.datetime.now()\n","print('Start:', start)\n","\n","trainer.fit(retrieval_module, data_module)\n","\n","# end timer and compute elapsed time\n","end = datetime.datetime.now()\n","print('End:', end)\n","elapsed_time = end - start\n","seconds = elapsed_time.total_seconds()\n","print('Elapsed Minutes: {}'.format(seconds / 60))\n","print('Elapsed Hours: {}'.format(seconds / 3600))"],"execution_count":null,"outputs":[]}]}